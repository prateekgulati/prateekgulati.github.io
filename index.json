[{"authors":["admin"],"categories":null,"content":"I am currently working as a Software Engineer II at Dell EMC. I am also enrolled in External Vision AI Program at The School of AI, Bangalore. My research interests include Deep Vision (Object Recognition, Detection, GAN, and VAE) and NLP.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently working as a Software Engineer II at Dell EMC. I am also enrolled in External Vision AI Program at The School of AI, Bangalore. My research interests include Deep Vision (Object Recognition, Detection, GAN, and VAE) and NLP.","tags":null,"title":"Prateek Gulati","type":"authors"},{"authors":[],"categories":[],"content":" EIP phase-1 Project Goal: Achieve atleast 60% validation accuracy on Tiny-Imagenet Dataset in less than 200 epochs.\nChallenges:\n Due to lack of infrastructure, google colab was the only option available.\n  Approach:\n Trained a custom made DenseNet Model (21 layers) after taking receptive field and object size into consideration.\n The model has 5 DenseBlocks each with 3 convolution layers and 4 transition blocks in between. There is no 5th transition block. Substitued Flattern + Dense with Global Average Pooling layer to enable progeressive resizing.\n Trained the model on scaled down images (16x16 and 32x32) for initial few epochs to extract the essential features and later on original size to improve the further accuracy.\n Train additionally on poorly performing classes identified using F1 score.\n Triangular Cyclic LR for faster convergence.\n 14 different type of image augmentation techniques.\n  Details:\n Batch size: 320\n Total Parameters: 8.9M\n Learning Rate: MaxLR = 0.05, MinLR=0.001 and StepSize=2000\n  Results:\n Validation Accuracy 62.95% Epochs 95  You can find the notebook here\n","date":1571759510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571759510,"objectID":"ed974de1a9b0b19b1f75a6d570b12547","permalink":"/project/tinyimagenet/","publishdate":"2019-10-22T21:21:50+05:30","relpermalink":"/project/tinyimagenet/","section":"project","summary":"Modified version of DenseNet architecture to train on tiny-ImageNet dataset.","tags":[],"title":"Tiny-Imagenet Challenge","type":"project"},{"authors":["Prateek Gulati"],"categories":[],"content":"","date":1569311555,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569311555,"objectID":"ad245ebea6a8e18392108c1673a115e7","permalink":"/post/batchnorm/","publishdate":"2019-09-24T13:22:35+05:30","relpermalink":"/post/batchnorm/","section":"post","summary":"This article discusses different types of normalizations in a Convolutional Neural Network and why Batch Normalization is necessary. There is an experiment that involves random dropping of BatchNorm Layers based on a hyperparameter in an Object Recognition model and validating it across the accuracy.","tags":["Machine Learning","Deep Learning","Batch Normalization","Convolutional Neural Net","Resnet"],"title":"Batchnormalization is not a Norm!","type":"post"},{"authors":["Prateek Gulati"],"categories":[],"content":"","date":1568188351,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568188351,"objectID":"8f5a823b5ff9a3f4a20664c320b39890","permalink":"/post/relu/","publishdate":"2019-09-11T13:22:31+05:30","relpermalink":"/post/relu/","section":"post","summary":"This article discusses the usage of ReLU in a Convolutional Neural Network. There is also an experiment that involves random dropping of Activation Layers based on a hyperparameter in an Object Recognition model and validating it across the accuracy.","tags":["Convolutional Neural Net","Deep Learning","Activation Functions","Resnet","Machine Learning"],"title":"Is ReLU ReLUvant?","type":"post"}]