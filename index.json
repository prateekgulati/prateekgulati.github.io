[{"authors":["admin"],"categories":null,"content":"I am currently working as a Software Engineer II at Dell EMC. I am also enrolled in External Vision AI Program at The School of AI, Bangalore. My primary research interests include Object Detection, Recognition, and generative models like GANs and Autoencoders. I am looking forward to working on developing efficient and adaptive models that are scalable and generic, hence, bridge-in the gap between academia and industry.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently working as a Software Engineer II at Dell EMC. I am also enrolled in External Vision AI Program at The School of AI, Bangalore. My primary research interests include Object Detection, Recognition, and generative models like GANs and Autoencoders. I am looking forward to working on developing efficient and adaptive models that are scalable and generic, hence, bridge-in the gap between academia and industry.","tags":null,"title":"Prateek Gulati","type":"authors"},{"authors":["Monimala Nej","A. Satyanarayana Reddy"],"categories":[],"content":"","date":1576371477,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576371477,"objectID":"7992a383f6bffbfe280b2b569ac2fc58","permalink":"/publication/binarysequence/","publishdate":"2019-12-15T06:27:57+05:30","relpermalink":"/publication/binarysequence/","section":"publication","summary":"Acknowledged for building a python algorithm to compute large values of 'n' and recognize patterns.","tags":[],"title":"Binary strings of length 'n' with 'x' zeros and longest 'k'-runs of zero","type":"publication"},{"authors":["Prateek Gulati"],"categories":[],"content":"","date":1569311555,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569311555,"objectID":"ad245ebea6a8e18392108c1673a115e7","permalink":"/post/batchnorm/","publishdate":"2019-09-24T13:22:35+05:30","relpermalink":"/post/batchnorm/","section":"post","summary":"This article discusses different types of normalizations in a Convolutional Neural Network and why Batch Normalization is necessary. There is an experiment that involves random dropping of BatchNorm Layers based on a hyperparameter in an Object Recognition model and validating it across the accuracy.","tags":["Machine Learning","Deep Learning","Batch Normalization","Convolutional Neural Net","Resnet"],"title":"Batchnormalization is not a Norm!","type":"post"},{"authors":["Prateek Gulati"],"categories":[],"content":"","date":1568188351,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568188351,"objectID":"8f5a823b5ff9a3f4a20664c320b39890","permalink":"/post/relu/","publishdate":"2019-09-11T13:22:31+05:30","relpermalink":"/post/relu/","section":"post","summary":"This article discusses the usage of ReLU in a Convolutional Neural Network. There is also an experiment that involves random dropping of Activation Layers based on a hyperparameter in an Object Recognition model and validating it across the accuracy.","tags":["Convolutional Neural Net","Deep Learning","Activation Functions","Resnet","Machine Learning"],"title":"Is ReLU ReLUvant?","type":"post"},{"authors":["Prateek Gulati"],"categories":[],"content":"","date":1564152227,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564152227,"objectID":"59b9b1db5d40880f14d715fa3390b971","permalink":"/project/cyclegan/","publishdate":"2019-07-26T20:13:47+05:30","relpermalink":"/project/cyclegan/","section":"project","summary":"Given a facial image, this can generate 24 different images by altering the race, gender and age of the person.","tags":["GAN","Deep Learning","cycle-GAN","Adversarial Network","Computer Vision"],"title":"Facial feature manipulation using Adversarial Networks","type":"project"},{"authors":["Prateek Gulati"],"categories":[],"content":" Goal: Achieve 94% validation accuracy on CIFAR10 Dataset in less than 100 secs on V100 GPU.\nChallenges:\n Due to lack of infrastructure, google colab was the only option available. To simulate the same environment, target is 94% validation accuracy in 600 secs on K80  Experiments: Experiment 1: Trained a custom made ResNet9 Model using tensorflow.keras.\nResults: Validation Accuracy: 92.2% Time: 1493 secs\nExperiment 2: Added a slanted one cycle Learning rate with gradual drop towards the end.\nResults: Validation Accuracy: 92.6% Time: 1502 secs\nExperiment 3: Added Image Augmentation: FlipLR, RandomPadCrop(padding of 4) and Cutout(16x16).\nResults: Validation Accuracy: 93.8% Time: 1627 secs\nExperiment 4: Built a pipeline using tfRecords and enabled prefetch for CPU and GPU to work in parallel.\nResults: Validation Accuracy: 93.8% Time: 741 secs\nExperiment 5: Augmented the data and then stored in tfRecords.\nResults: Validation Accuracy: 93.8% Time: 602 secs\nDetails:\n Batch size: 512\n Total Parameters: 8.9M (check params)\n Learning Rate: MaxLR = 0.4 at epoch 5, MinLR=0.001 at epoch 20 and gradual drop 0.0001 at 24th epoch  Results:\n Validation Accuracy 93.80% Time: 602 secs Epochs 24  ","date":1561387409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561387409,"objectID":"e33de16d816cfcdeb7e4beb96bb74106","permalink":"/project/dawnbench/","publishdate":"2019-06-24T20:13:29+05:30","relpermalink":"/project/dawnbench/","section":"project","summary":"An attempt to beat DawnBench using modified Resnet9","tags":["Deep Learning","State of The Art","Computer Vision","Stanford"],"title":"Stanford DawnBench - CIFAR10","type":"project"},{"authors":["Prateek Gulati"],"categories":[],"content":" EIP phase-1 Project Goal: Achieve atleast 60% validation accuracy on Tiny-Imagenet Dataset in less than 200 epochs.\nChallenges:\n Due to lack of infrastructure, google colab was the only option available.\n  Approach:\n Trained a custom made DenseNet Model (21 layers) after taking receptive field and object size into consideration.\n The model has 5 DenseBlocks each with 3 convolution layers and 4 transition blocks in between. There is no 5th transition block. Substitued Flattern + Dense with Global Average Pooling layer to enable progeressive resizing.\n Trained the model on scaled down images (16x16 and 32x32) for initial few epochs to extract the essential features and later on original size to improve the further accuracy.\n Train additionally on poorly performing classes identified using F1 score.\n Triangular Cyclic LR for faster convergence.\n 14 different type of image augmentation techniques.\n  Details:\n Batch size: 320\n Total Parameters: 8.9M\n Learning Rate: MaxLR = 0.05, MinLR=0.001 and StepSize=2000\n  Results:\n Validation Accuracy 62.95% Epochs 95  You can find the notebook here\n","date":1551196310,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551196310,"objectID":"ed974de1a9b0b19b1f75a6d570b12547","permalink":"/project/tinyimagenet/","publishdate":"2019-02-26T21:21:50+05:30","relpermalink":"/project/tinyimagenet/","section":"project","summary":"Modified version of DenseNet architecture to train on tiny-ImageNet dataset.","tags":["Computer Vision","Convolutional Neural Network","Deep Learning"],"title":"Tiny-Imagenet Challenge","type":"project"}]